# -*- coding: utf-8 -*-
"""Naive Bayes IMDB Dataset - Mihir Kawatra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lGkBdyKkJ5xEDrQBk5ypEFzAScBXgNci

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB, ComplementNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split, GridSearchCV

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
stop_words = set(stopwords.words('english')) 

import re
from tqdm import tqdm
tqdm.pandas()

"""# Data Prep"""
DATA_DIR = './data/'
data = pd.read_csv(DATA_DIR+'IMDB Dataset.csv')

data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})

X = data['review']
y = data['sentiment']

TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return TAG_RE.sub('', text)

X = X.progress_apply(lambda x: remove_tags(x))

x_train, x_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.2)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""# Modelling"""

vect = CountVectorizer(stop_words = stop_words, ngram_range=(2,2))
x_train_dtm = vect.fit_transform(x_train)
x_test_dtm = vect.transform(x_test)

nb = MultinomialNB()
nb.fit(x_train_dtm, y_train)
y_pred = nb.predict(x_test_dtm)
print(f"Testing Accuracy: {round(accuracy_score(y_test, y_pred)*100, 2)}%")

"""## Optimization and Tuning"""

text_clf = Pipeline([('tfidf', TfidfTransformer()),
                     ('clf', 'passthrough')])
tuned_parameters = {
    'tfidf__use_idf': (True, False),
    'tfidf__norm': ('l1', 'l2'),
    'clf__alpha': [1, 1e-1, 1e-2],
    'clf': [MultinomialNB(), BernoulliNB(), ComplementNB()]
}
grid = GridSearchCV(text_clf, tuned_parameters, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)
grid.fit(x_train_dtm, y_train)

print(f"Testing Accuracy: {round(grid.best_estimator_.score(x_test_dtm, y_test)*100, 2)}%")

print(classification_report(y_test, grid.best_estimator_.predict(x_test_dtm)))

y_score = grid.best_estimator_.predict_proba(x_test_dtm)
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_score[:,1])
roc_auc = auc(false_positive_rate, true_positive_rate)

plt.title('ROC')
plt.plot(false_positive_rate, true_positive_rate, c='darkorange', label=('AUC= %0.2f'%roc_auc))
plt.legend(loc='lower right', prop={'size':8})
plt.plot([0,1],[0,1], color='blue', linestyle='--')
plt.xlim([-0.05,1.0])
plt.ylim([0.0,1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()